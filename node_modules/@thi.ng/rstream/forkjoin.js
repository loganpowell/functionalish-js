import { comp, map, mapcat, range } from "@thi.ng/transducers";
import { sync } from "./stream-sync";
import { tunnel } from "./subs/tunnel";
/**
 * Returns a {@link StreamSync} instance which creates & attaches
 * multiple subscriptions to given `src` input stream, processes each
 * received value in parallel via web workers, then recombines partial
 * results and passes the resulting transformed value downstream.
 *
 * @remarks
 * See {@link ForkJoinOpts} for further details & behavior options and
 * the {@link forkBuffer} and {@link joinBuffer} helpers for array-based
 * value processing (most likely use case).
 *
 * @param src - input stream
 * @param opts -
 */
export const forkJoin = (opts) => {
    const numWorkers = opts.numWorkers || navigator.hardwareConcurrency || 4;
    const workerIDs = range(numWorkers);
    return sync({
        src: [
            ...map((id) => opts.src
                .transform(map((x) => opts.fork(id, numWorkers, x)))
                .subscribe(tunnel({
                src: opts.worker,
                transferables: opts.transferables,
                interrupt: opts.interrupt === true,
                terminate: opts.terminate,
                id: String(id)
            })), workerIDs)
        ],
        xform: comp(
        // form result tuple in original order
        map((results) => [...map((id) => results[id], workerIDs)]), 
        // apply user join function
        map(opts.join)),
        reset: true,
        backPressure: opts.backPressure
    });
};
/**
 * Higher-order fork function for scenarios involving the split-parallel
 * processing of a large buffer.
 *
 * @remarks
 * The returned function is meant to be used as `fork` function in a
 * {@link ForkJoinOpts} config and extracts a workload slice of the
 * original buffer for a single worker. The HOF itself takes a minimum
 * chunk size as optional parameter (default: 1).
 *
 * **Note:** Depending on the configured `minChunkSize` and the size of
 * the input buffer to be partitioned, the returned fork function might
 * produce empty sub-arrays for some workers, iff the configured number
 * of workers exceeds the resulting number of chunks / input values.
 * E.g. If the number of workers = 8, buffer size = 10 and min chunk
 * size = 2, then the last 3 (i.e. 8 - 10 / 2) workers will only receive
 * empty workloads.
 *
 * More generally, if the input buffer size is not equally distributable
 * over the given number of workers, the last worker(s) might receive a
 * larger, smaller or empty chunk.
 *
 * Also see {@link forkJoin} and {@link joinBuffer}.
 *
 * @example
 * ```ts
 * forkJoin<number[], number[], number[], number[]>({
 *     src,
 *     // job definition / split buffer into chunks (min size 256 values)
 *     fork: forkBuffer(256),
 *     // re-join partial results
 *     join: joinBuffer(),
 *     worker: "./worker.js",
 * })
 * ```
 *
 * @param minChunkSize -
 */
export const forkBuffer = (minChunkSize = 1) => (id, numWorkers, buf) => {
    const chunkSize = Math.max(minChunkSize, (buf.length / numWorkers) | 0);
    return id < numWorkers - 1
        ? buf.slice(id * chunkSize, (id + 1) * chunkSize)
        : buf.slice(id * chunkSize);
};
/**
 * Higher-order join function for scenarios involving the split-parallel
 * processing of a large buffer.
 *
 * @remarks
 * The returned function is meant to be used as `join` function in a
 * {@link ForkJoinOpts} config, receives the processed result chunks
 * from all workers (ordered by worker ID) and concatenates them back
 * into a single result array.
 *
 * The optional `fn` arg can be used to pick the actual result chunk
 * from each worker result. This is useful if the worker result type is
 * not an array and includes other data points (e.g. execution metrics
 * etc.). If `fn` is not given, it defaults to the identity function
 * (i.e. each worker's result is assumed to be an array).
 *
 * Also see {@link forkJoin} and {@link forkBuffer}.
 *
 * @param fn -
 */
export const joinBuffer = (fn) => fn
    ? (parts) => [...mapcat(fn, parts)]
    : (parts) => Array.prototype.concat.apply([], parts);
